# -*- coding: utf-8 -*-
"""ML Assignment 01 Problem 02 Classification 24K-7606

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mnMt0kl5U2rCyIWzuqVHHG_TDMVZnRcZ

## AI5003 - Advance Machine Learning - Assignment No. 1 - Problem 02 - (Classification)
## Name: **Muhammad Azhar**
## ID: **24K-7606**
## Submitted to: **Professor Dr. Muhammad Rafi**

Setup and data loading
"""

# Install required packages
!pip install ydata-profiling -q

# Import necessary libraries
from ydata_profiling import ProfileReport
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split , cross_val_score
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Load the dataset
df = pd.read_csv('dataset for A1P2 drug200.csv')

# Generate a profile report for detailed data exploration
profile = ProfileReport(df)
profile.to_file(output_file="startup-data-analytics.html")

"""Data exploration and preprocessing"""

df.head() # Display first few rows of the dataset

df.info() # Check data information (data types, non-null values)

df.isnull().sum() # Check for missing values

# Check unique values in categorical columns
print("Unique values in categorical columns:")
print("Sex categories:", df['Sex'].unique())
print("Blood Pressure categories:", df['BP'].unique())
print("Cholesterol categories:", df['Cholesterol'].unique())
print("Drug categories:", df['Drug'].unique())

"""Exploratory Data Analysis (EDA)"""

# Visualize the distribution of the target variable (Drug)
plt.figure(figsize=(10, 3))
drug_counts = df['Drug'].value_counts()
sns.barplot(x=drug_counts.index, y=drug_counts.values,color='green')
plt.title("Distribution of Drug Categories")
plt.xlabel("Drug Type")
plt.ylabel("Count")
plt.show()

# Visualize age distribution
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(df["Age"], bins=20, kde=True,color='green')
plt.title("Age Distribution")
plt.xlabel("Age")
plt.ylabel("Frequency")

plt.subplot(1, 2, 2)
sns.boxplot(x="Drug", y="Age", data=df,color='green')
plt.title("Age by Drug Category")
plt.tight_layout()
plt.show()

# Visualize Na_to_K distribution
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
sns.histplot(df['Na_to_K'], bins=30, kde=True,color='green')
plt.title("Distribution of Na_to_K Ratio")
plt.xlabel("Na_to_K")
plt.ylabel("Frequency")

plt.subplot(1, 2, 2)
sns.boxplot(x="Drug", y="Na_to_K", data=df,color='green')
plt.title("Na_to_K Ratio by Drug Category")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

df['Na_to_K'].describe()

plt.figure(figsize=(10, 3))
sns.histplot(df['Na_to_K'], bins=30, kde=True,color='green')
plt.xlabel("Na_to_K")
plt.ylabel("Frequency")
plt.title("Distribution of Na_to_K")
plt.show()

"""Relationship between categorical variables and drug category"""

plt.figure(figsize=(18, 5))

plt.subplot(1, 3, 1)
sns.countplot(x='Drug', hue='Sex', data=df,palette='dark:green')
plt.title("Drug Distribution by Sex")
plt.xlabel("Drug")
plt.ylabel("Count")

plt.subplot(1, 3, 2)
custom_palette = ["red", "green", "black"]
sns.countplot(x='Drug', hue='BP', data=df,palette=custom_palette)
plt.title("Drug Distribution by Blood Pressure")
plt.xlabel("Drug")
plt.ylabel("Count")

plt.subplot(1, 3, 3)
sns.countplot(x='Drug', hue='Cholesterol', data=df,palette='dark:green')
plt.title("Drug Distribution by Cholesterol")
plt.xlabel("Drug")
plt.ylabel("Count")

plt.tight_layout()
plt.show()

df.describe(include='all') # Statistical summary of the dataset

"""Data Preprocessing"""

df = pd.get_dummies(df, columns=['Sex', 'BP', 'Cholesterol'], drop_first=True) # Encode categorical variables using one-hot encoding

drug_mapping = {
    'drugY': 0,
    'drugC': 1,
    'drugX': 2,
    'drugA': 3,
    'drugB': 4
}
df['Drug_encoded'] = df['Drug'].map(drug_mapping)
# Map drug categories to numerical values

# Prepare features and target variable
x = df.drop(['Drug', 'Drug_encoded'], axis=1)
y = df['Drug_encoded']

# Normalize numerical features (Age and Na_to_K)
scaler = StandardScaler()
x[['Age', 'Na_to_K']] = scaler.fit_transform(x[['Age', 'Na_to_K']])

x_train , x_test , y_train , y_test = train_test_split( x , y , test_size = 0.20 , random_state = 1)

print(f"Training set size: {x_train.shape[0]} samples")
print(f"Testing set size: {x_test.shape[0]} samples")

"""Decision Tree Model"""

# Initialize and train the Decision Tree model
dt_classifier = DecisionTreeClassifier(max_depth=3, random_state=42)
dt_classifier.fit(x_train, y_train)

dt_y_pred = dt_classifier.predict(x_test) # Make predictions

print("Accuracy:", metrics.accuracy_score(y_test, dt_y_pred))

print("\nClassification Report:")
print(classification_report(y_test, dt_y_pred,
                           target_names=['drugY', 'drugC', 'drugX', 'drugA', 'drugB'],
                           zero_division=0))

#prompt create confusion metrics for secision tree
print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, dt_y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['drugY', 'drugC', 'drugX', 'drugA', 'drugB'],
            yticklabels=['drugY', 'drugC', 'drugX', 'drugA', 'drugB'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Decision Tree')
plt.show()

# Cross-validation for Decision Tree
dt_cv_scores = cross_val_score(dt_classifier, x, y, cv=5)
print(f"\nCross-Validation Scores (5-fold): {dt_cv_scores}")
print(f"Average CV Score: {dt_cv_scores.mean():.4f} ± {dt_cv_scores.std():.4f}")

# Visualize the Decision Tree
plt.figure(figsize=(10, 6))
tree.plot_tree(dt_classifier,
               feature_names=x.columns,
               class_names=['drugY', 'drugC', 'drugX', 'drugA', 'drugB'],
               filled=True,
               rounded=True)
plt.title("Decision Tree Visualization")
plt.show()

# Feature importance for Decision Tree
dt_feature_imp = pd.Series(dt_classifier.feature_importances_, index=x.columns).sort_values(ascending=False)
plt.figure(figsize=(8, 3))
dt_feature_imp.plot.bar(color='green')
plt.title('Feature Importance in Decision Tree Model')
plt.ylabel('Feature Importance Score')
plt.tight_layout()
plt.show()

"""Random Forest Model"""

# Initialize and train the Random Forest model
rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=42)
rf_classifier.fit(x_train, y_train)

# Make predictions
rf_y_pred = rf_classifier.predict(x_test)

print("Accuracy:", metrics.accuracy_score(y_test, rf_y_pred))

print("\nClassification Report:")
print(classification_report(y_test, rf_y_pred,
                           target_names=['drugY', 'drugC', 'drugX', 'drugA', 'drugB']))

# Confusion matrix
print("\nConfusion Matrix:")
cm = confusion_matrix(y_test, rf_y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=['drugY', 'drugC', 'drugX', 'drugA', 'drugB'],
            yticklabels=['drugY', 'drugC', 'drugX', 'drugA', 'drugB'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix - Random Forest')
plt.show()

# Cross-validation for Random Forest
rf_cv_scores = cross_val_score(rf_classifier, x, y, cv=5)
print(f"\nCross-Validation Scores (5-fold): {rf_cv_scores}")
print(f"Average CV Score: {rf_cv_scores.mean():.4f} ± {rf_cv_scores.std():.4f}")

#Prompt : Plot the Random Forest
# Access the first tree in the forest (adjust index if needed)
plt.figure(figsize=(10, 12))
tree.plot_tree(rf_classifier.estimators_[0],
               feature_names=x.columns,
               class_names=['drugY', 'drugC', 'drugX', 'drugA', 'drugB'],
               filled=True,
               rounded=True)
plt.title("Sample Tree from Random Forest")
plt.show()

# Feature importance for Random Forest
rf_feature_imp = pd.Series(rf_classifier.feature_importances_, index=x.columns).sort_values(ascending=False)
plt.figure(figsize=(8, 5))
rf_feature_imp.plot.bar(color='green')
plt.title('Feature Importance in Random Forest Model')
plt.ylabel('Feature Importance Score')
plt.tight_layout()
plt.show()

"""Model Comparison And Conclusions"""

models = ['Decision Tree', 'Random Forest']
accuracies = [metrics.accuracy_score(y_test, dt_y_pred), metrics.accuracy_score(y_test, rf_y_pred)]
cv_scores = [dt_cv_scores.mean(), rf_cv_scores.mean()]

plt.figure(figsize=(4, 5))
bar_width = 0.35
x = np.arange(len(models))
plt.bar(x - bar_width/2, accuracies, bar_width, label='Test Accuracy',color='green')
plt.bar(x + bar_width/2, cv_scores, bar_width, label='CV Accuracy',color='black')
plt.ylabel('Accuracy')
plt.title('Model Performance Comparison')
plt.xticks(x, models)
plt.legend()
plt.ylim(0, 1)
plt.tight_layout()
plt.show()

# Compare feature importance between models
plt.figure(figsize=(14, 7))

plt.subplot(1, 2, 1)
dt_feature_imp.plot.bar(color='green')
plt.title('Feature Importance - Decision Tree')
plt.tight_layout()

plt.subplot(1, 2, 2)
rf_feature_imp.plot.bar(color='blue')
plt.title('Feature Importance - Random Forest')
plt.tight_layout()

plt.show()

#Prompt help me write report on influential features , comparing both , insights

"""# Model Comparison and Analysis

## Comparing Models: Decision Tree vs Random Forest

**Random Forest outperforms the Decision Tree model**:

1. **Accuracy**:
   - Decision Tree: 85%
   - Random Forest: 100%

2. **Class-specific Performance**:
   - Decision Tree struggles with minority classes (drugC and drugB have 0% precision and recall)
   - Random Forest perfectly classifies all drug categories, even minority classes

3. **Consistency**:
   - Decision Tree CV score: 0.83 ± 0.019
   - Random Forest CV score: 0.975 ± 0.039
   
4. **Error Analysis**:
   - Decision Tree confusion matrix shows all drugC samples were misclassified as drugX, and all drugB samples were misclassified as drugA
   - Random Forest correctly classified all samples across all categories

## Most Influential Features

1. **Na_to_K ratio** is by far the most important feature for both models:
   - Decision Tree: 64.3% importance
   - Random Forest: 62.9% importance
   
2. **Blood Pressure** is the second most important feature:
   - Combined importance (BP_LOW + BP_NORMAL) in Decision Tree: ~35.7%
   - Combined importance in Random Forest: ~22.3%
   
## Challenges Encountered and Solutions

1. **Class Imbalance**:
   - Challenge: The dataset has uneven distribution across drug categories (17 samples of drugY vs only 2 samples of drugB)
   - Solution: Random Forest naturally handled this imbalance better through its ensemble approach and bootstrapping

## Recommendations for Model Improvement
   
1. **Handling Class Imbalance**:
   - Apply techniques like SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples for minority classes
   
2. **Additional Data Collection**:
   - More samples for underrepresented classes (drugB, drugC) would improve model robustness
   
6. **Feature Expansion**:
   - Collect additional health markers that might have predictive power for drug classification

In conclusion, while both models provide valuable insights, Random Forest demonstrates superior performance for this classification task, particularly in handling minority classes and utilizing the full range of available features.


"""